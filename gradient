#include <iostream>
#include <math.h>
using namespace std;



// calculate alternation rate for an edge weight between a neuron in 
// the output layer and another one in hidden layer

double gradient_output_layer(double eta, double target, double actual, double input){

    // activation function:
    double activation_function = pow(1 + exp(-actual), -1);

    // derivative:
    double derivative = exp(-actual) / pow(exp(-actual) + 1, 2);
    double gradient = 2 * eta * (target - activation_function) * derivative * input;

    return gradient;
}



// calculate alternation rate for an edge between a neuron in an input layer (which also can be a hidden layer) and a
// hidden layer. The three vectors given as parameters are:
// target_values = the values the neural net should have as output for the state it is in
// actual_values = the output values of the neural net
// weights = the edge weights between on neuron in the hidden layer to all the neurons in the output layer

double gradient_hiddenlayer(vector<double> target_values, vector<double> actual_values, vector<double> weights,
        double eta, double target, double actual, double input){

    double sum = 0;
    for(int i = 0; i < target_values.size(); i++) {
        double error_rate = target_values[i] - pow(1 + exp(-actual_values[i]), -1)
                * (exp(-actual_values[i]) / pow(exp(-actual_values[i]) + 1, 2))
                    * weights[i];

        sum += error_rate;
    }
    // derivative:
    double derivative = exp(-actual) / pow(exp(-actual) + 1, 2);
    double gradient = 2 * eta * sum * derivative * input;
    
    return gradient;
}



